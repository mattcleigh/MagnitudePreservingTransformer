_target_: src.models.gpt.GPT
vocab_size: 50257
dim: 512
num_layers: 8
max_seq_len: ${datamodule.max_seq_len}
final_norm: rms
layer_config:
  num_heads: 8
  causal: True
  pre_norm: rms
  qk_norm: rms
  out_norm: rms
optimizer:
  _target_: src.torch_utils.AdamWS
  _partial_: True
  lr: 5e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
scheduler:
  _target_: src.schedulers.linear_warmup_cosine_decay
  _partial_: True
  warmup_steps: 1000
  total_steps: 1000_000
