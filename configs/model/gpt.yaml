_target_: src.models.gpt.GPT
vocab_size: 50257
dim: 512
num_layers: 8
final_norm: rms
layer_config:
  num_heads: 16
  ff_mult: 2
  pre_norm: rms
  qk_norm: rms
  out_norm: rms
optimizer:
  _target_: src.torch_utils.AdamWS
  _partial_: True
  lr: 5.0e-4
  betas: [0.9, 0.95]
  weight_decay: 0.1
scheduler:
  _target_: src.schedulers.linear_warmup_cosine_decay
  _partial_: True
  warmup_steps: 1_000
  total_steps: 200_000
