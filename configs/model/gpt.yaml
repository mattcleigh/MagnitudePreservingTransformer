_target_: src.models.gpt.GPT
vocab_size: 50257
dim: 1024
num_layers: 8
final_norm: rms
layer_config:
  num_heads: 16
  pre_norm: rms
  qk_norm: rms
  out_norm: rms
optimizer:
  _target_: src.torch_utils.AdamWS
  _partial_: True
  lr: 1e-3
  betas: [0.9, 0.95]
  weight_decay: 0.1
scheduler:
  _target_: src.schedulers.linear_warmup_cosine_decay
  _partial_: True
  warmup_steps: 1000
  total_steps: 1000_000
