# @package _global_

# Order indicates overwriting
defaults:
  - _self_
  - hydra: default
  - datamodule: openwebtext
  - experiment: null

seed: 42 # For reproducibility
project_name: gpt # Determines output directory path and wandb project
network_name: gpt # Used for both saving and wandb
output_dir: /srv/beegfs/scratch/groups/rodem/nlp/
ckpt_path: null  # Checkpoint path to resume training
weight_ckpt_path: null # Checkpoint path to load weights (but not optimizers etc)

# Extra tweaks available with the new pytorch version
precision: medium # Should use medium if on ampere gpus
compile: null # Can set to default for faster compiles
tags: null # Extra tags passed to the logger

# COMPLETELY replaces the all config info with what is contained in ${full_path}
# This is ideal for resuming a job, log to the same directory
# Will also resume the loggers and set the ckpt_path to the latest
full_resume: False
ckpt_flag: last.ckpt # Name of the checkpoint file, can use wildcards

# Model settings
model:
  _target_: src.models.gpt.GPT
  vocab_size: 50257
  dim: 512
  num_layers: 12
  max_seq_len: ${datamodule.max_seq_len}
  final_norm: rms
  layer_config:
    num_heads: 8
    drop: 0
    causal: True
    pre_norm: rms
    qk_norm: rms
    out_norm: rms
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: True
    lr: 5e-4
    betas: [0.9, 0.95]
    weight_decay: 0.1
  scheduler:
    _target_: src.schedulers.linear_warmup_cosine_decay
    _partial_: True
    warmup_steps: 1000
    total_steps: 1000_000

# Trainer settings
trainer:
  _target_: lightning.Trainer
  max_epochs: 999999
  enable_progress_bar: True
  gradient_clip_val: 1
  precision: 16-mixed
  check_val_every_n_epoch: 1
  accelerator: auto
  devices: 1
  num_nodes: 1
  default_root_dir: ${full_path}

# Logger settings
logger:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  offline: False
  id: null
  log_model: False
  tags: ${tags}
  project: ${project_name}
  name: ${network_name}
  save_dir: ${full_path}
  resume: ${full_resume}

# Callbacks
callbacks:
  checkpoint_per_epoch:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${full_path}/checkpoints
    filename: last
    enable_version_counter: False
    auto_insert_metric_name: False
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: 2
  lr_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step

# Interpolated paths
root_dir: ${oc.env:PROJECT_ROOT}
full_path: ${output_dir}/${project_name}/${network_name}/

